{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'dp'\n",
    "require 'hdf5'\n",
    "require 'optim'\n",
    "require 'gnuplot' --or 'image'\n",
    "\n",
    "path = '/Users/shima/Documents/MyQuarters/Quarter14/CS231n/final_project/CS231n/dataset/data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function load_data(data_file)\n",
    "  local f = hdf5.open(data_file)\n",
    "  local dset = {}\n",
    "  dset.X_train = f:read('X_train'):all()\n",
    "  dset.y_train = f:read('y_train'):all() +1\n",
    "  dset.X_val = f:read('X_val'):all()\n",
    "  dset.y_val = f:read('y_val'):all() + 1\n",
    "  dset.X_test = f:read('X_test'):all()\n",
    "  dset.y_test = f:read('y_test'):all() + 1\n",
    "  f:close()\n",
    "  print('Data is loaded, Ready to go!')\n",
    "    \n",
    "  return dset\n",
    "end\n",
    "\n",
    "--[[ preprocess data before feeding that into the convnet\n",
    "     subtract mean of training data, and reshape it to the\n",
    "     form (N, 1, H, W) ]]--\n",
    "function preprocess_data(dset)\n",
    "\n",
    "    dset_new = {}\n",
    "    \n",
    "    -- subtract mean\n",
    "\tmean_image = torch.mean(dset.X_train)\n",
    "\tdset_new.X_train = dset.X_train - mean_image\n",
    "\tdset_new.X_val = dset.X_val - mean_image\n",
    "\tdset_new.X_test = dset.X_test - mean_image\n",
    "\n",
    "\t-- reshape\n",
    "\tH = dset.X_train:size(2)\n",
    "\tW = dset.X_train:size(3)\n",
    "\t\n",
    "\tdset_new.X_train = torch.reshape(dset_new.X_train, dset.X_train:size(1), 1, H, W)\n",
    "\tdset_new.X_val = torch.reshape(dset_new.X_val, dset.X_val:size(1), 1, H, W)\n",
    "\tdset_new.X_test = torch.reshape(dset_new.X_test, dset.X_test:size(1), 1, H, W)\n",
    "    \n",
    "    dset_new.y_train = dset.y_train\n",
    "    dset_new.y_val = dset.y_val\n",
    "    dset_new.y_test = dset.y_test\n",
    "\n",
    "    print('data preprocessing done.')\n",
    "\treturn dset_new\n",
    "end\n",
    "\n",
    "function get_minibatch(X, y, batch_size)\n",
    "  local mask = torch.LongTensor(batch_size):random(X:size(1))\n",
    "  local X_batch = X:index(1, mask)\n",
    "  local y_batch = y:index(1, mask)\n",
    "  return X_batch, y_batch\n",
    "end\n",
    "\n",
    "function check_accuracy(X, y, model, batch_size)\n",
    "    \n",
    "  model:evaluate()\n",
    "  local num_correct = 0\n",
    "  local num_tested = 0\n",
    "    \n",
    "  for t = 1, 20 do\n",
    "    local X_batch, y_batch = get_minibatch(X, y, batch_size)\n",
    "    -- X_batch = center_crop(X_batch)\n",
    "    X_batch = X_batch:cuda()\n",
    "    y_batch = y_batch:cuda()\n",
    "    local scores = model:forward(X_batch)\n",
    "    local _, y_pred = scores:max(2)\n",
    "    num_correct = num_correct + torch.eq(y_pred, y_batch):sum()\n",
    "    num_tested = num_tested + batch_size\n",
    "  end\n",
    "  return num_correct / num_tested\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data is loaded, Ready to go!\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "data preprocessing done.\t\n",
       "X_val\t"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " 3589\n",
       "    1\n",
       "   48\n",
       "   48\n",
       "[torch.LongStorage of size 4]\n",
       "\n",
       "X_train\t 28709\n",
       "     1\n",
       "    48\n",
       "    48\n",
       "[torch.LongStorage of size 4]\n",
       "\n",
       "X_test\t 3589\n",
       "    1\n",
       "   48\n",
       "   48\n",
       "[torch.LongStorage of size 4]\n",
       "\n",
       "y_train\t 28709\n",
       "[torch.LongStorage of size 1]\n",
       "\n",
       "y_test\t 3589\n",
       "[torch.LongStorage of size 1]\n",
       "\n",
       "y_val\t 3589\n",
       "[torch.LongStorage of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Now let's load the data\n",
    "dset = load_data(path)\n",
    "dset = preprocess_data(dset)\n",
    "\n",
    "-- Print data size and shape\n",
    "for k, v in pairs(dset) do\n",
    "  print(k, v:size())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--build a convnet model\n",
    "function full_conv_net(convlayer_params, affinelayer_params)\n",
    "  \n",
    "  local num_filters = convlayer_params['num_filters'] --{64,  64,  128,  128,  256, 256, 512, 512, 1024}\n",
    "  local filter_sizes =  convlayer_params['filter_size'] --{5,   3,   3,   3,   3,   3,   3,   3,   3}\n",
    "  local filter_strides = convlayer_params['stride'] --{2,   1,   2,   1,   2,   1,   2,   1,   2}\n",
    "  local use_sbatchnorm = convlayer_params['s_batch_norm']\n",
    "  local maxpool_dim = convlayer_params['pool_dims']\n",
    "  local maxpool_stride = convlayer_params['pool_strides']\n",
    "  \n",
    "  local hidden_dims = affinelayer_params['hidden_dims']\n",
    "  local use_batchnorm = affinelayer_params['batch_norm']\n",
    "  local use_dropout = affinelayer_params['dropout']\n",
    "    \n",
    "  local num_classes = 7\n",
    "  -- C: number of channels , H,W: height and width of an image\n",
    "  \n",
    "  local C, H, W = 1, 48, 48 \n",
    "  local image_size = 48\n",
    "  local prev_dim = 3\n",
    "  local cur_size = image_size\n",
    "  \n",
    "  local next_C = C\n",
    "  local next_H = H\n",
    "  local next_W = W\n",
    "  -- generate the model\n",
    "  local model = nn.Sequential()\n",
    "  for i = 1, #num_filters do\n",
    "     \n",
    "    local zero_pad = (filter_sizes[i] - 1) / 2\n",
    "    model:add(nn.SpatialConvolution(next_C, num_filters[i], filter_sizes[i], filter_sizes[i], \n",
    "                filter_strides[i], filter_strides[i], zero_pad, zero_pad))\n",
    "        \n",
    "    next_C = num_filters[i]\n",
    "    next_W = (next_W + 2*zero_pad - filter_sizes[i]) / filter_strides[i] + 1\n",
    "    next_H = (next_H + 2*zero_pad - filter_sizes[i]) / filter_strides[i] + 1\n",
    "        \n",
    "    if use_sbatchnorm then\n",
    "        model:add(nn.SpatialBatchNormalization(next_C))\n",
    "    end\n",
    "            \n",
    "    model:add(nn.ReLU())\n",
    "    model:add(nn.SpatialMaxPooling(maxpool_dim, maxpool_dim, maxpool_stride, maxpool_stride))\n",
    "    \n",
    "    next_W = (next_W - maxpool_dim) / maxpool_stride + 1\n",
    "    next_H = (next_H - maxpool_dim) / maxpool_stride + 1\n",
    "  end\n",
    "    \n",
    "  local next_D = next_C * next_W * next_H\n",
    "  model:add(nn.View(-1):setNumInputDims(3))\n",
    "        \n",
    "  for i = 1, #hidden_dims do\n",
    "      model:add(nn.Linear(next_D, hidden_dims[i]))\n",
    "      next_D = hidden_dims[i]\n",
    "    \n",
    "      if use_batchnorm then\n",
    "          model:add(nn.BatchNormalization(hidden_dims[i]))\n",
    "      end\n",
    "    \n",
    "      if use_dropout then\n",
    "          model:add(nn.Dropout(0.5))\n",
    "      end\n",
    "                    \n",
    "      model:add(nn.ReLU())\n",
    "  end\n",
    "                \n",
    "  model:add(nn.Linear(next_D, num_classes))\n",
    "  \n",
    "  return model\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Build a sample model\n",
    "local convlayer_params = {['num_filters']= {32, 32}, ['filter_size']= {3, 3} ,['stride']={1, 1}, \n",
    "                          ['s_batch_norm']= false, ['pool_dims']= 2, ['pool_strides']= 2}\n",
    "local affinelayer_params = {['hidden_dims']= {50}, ['batch_norm']= false,['dropout']= false}\n",
    "\n",
    "model = full_conv_net(convlayer_params, affinelayer_params)\n",
    "model:training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan\t\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "/Users/shima/torch/install/share/lua/5.1/nn/THNN.lua:804: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/shima/torch/extra/nn/lib/THNN/generic/ClassNLLCriterion.c:41\nstack traceback:\n\t[C]: in function 'v'\n\t/Users/shima/torch/install/share/lua/5.1/nn/THNN.lua:804: in function 'ClassNLLCriterion_updateOutput'\n\t...ima/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'updateOutput'\n\t...torch/install/share/lua/5.1/nn/CrossEntropyCriterion.lua:13: in function 'forward'\n\t[string \"-- compute loss and gradients...\"]:22: in function 'opfunc'\n\t/Users/shima/torch/install/share/lua/5.1/optim/adam.lua:33: in function 'adam'\n\t[string \"-- compute loss and gradients...\"]:43: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/shima/torch/install/share/lua/5.1/itorch/main.lua:209: in function </Users/shima/torch/install/share/lua/5.1/itorch/main.lua:173>\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/shima/torch/install/share/lua/5.1/itorch/main.lua:381: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010c94bbd0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "/Users/shima/torch/install/share/lua/5.1/nn/THNN.lua:804: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/shima/torch/extra/nn/lib/THNN/generic/ClassNLLCriterion.c:41\nstack traceback:\n\t[C]: in function 'v'\n\t/Users/shima/torch/install/share/lua/5.1/nn/THNN.lua:804: in function 'ClassNLLCriterion_updateOutput'\n\t...ima/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'updateOutput'\n\t...torch/install/share/lua/5.1/nn/CrossEntropyCriterion.lua:13: in function 'forward'\n\t[string \"-- compute loss and gradients...\"]:22: in function 'opfunc'\n\t/Users/shima/torch/install/share/lua/5.1/optim/adam.lua:33: in function 'adam'\n\t[string \"-- compute loss and gradients...\"]:43: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/shima/torch/install/share/lua/5.1/itorch/main.lua:209: in function </Users/shima/torch/install/share/lua/5.1/itorch/main.lua:173>\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/shima/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/shima/torch/install/share/lua/5.1/itorch/main.lua:381: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010c94bbd0"
     ]
    }
   ],
   "source": [
    "-- compute loss and gradients\n",
    "crit = nn.CrossEntropyCriterion() \n",
    "local batch_size = 50\n",
    "local reg = 0\n",
    "local num_iterations = 10\n",
    "local config = {\n",
    "  learningRate=1e-2,\n",
    "}\n",
    "\n",
    "\n",
    "model:zeroGradParameters()\n",
    "local params, gradParams = model:getParameters()\n",
    "\n",
    "function f(w)\n",
    "   \n",
    "  local X_batch, y_batch = get_minibatch(dset.X_train, dset.y_train, batch_size)\n",
    "    \n",
    "  -- X_batch = X_batch:cuda() \n",
    "  --y_batch = y_batch:cuda() \n",
    "  assert(w == params)\n",
    "  local scores = model:forward(X_batch)\n",
    "  local data_loss = crit:forward(scores, y_batch)\n",
    "  local dscores = crit:backward(scores, y_batch)\n",
    "  model:backward(X_batch, dscores)\n",
    "  \n",
    "  -- add regularization\n",
    "  print(torch.sum(gradParams))\n",
    "  gradParams:add(reg, params)\n",
    "  print(torch.sum(gradParams))\n",
    "    \n",
    "  if t % 100 == 0 then\n",
    "    print(t, data_loss, torch.abs(gradParams):mean())\n",
    "  end\n",
    "  \n",
    "  return data_loss, gradParams\n",
    "end\n",
    "\n",
    "-- optimization process\n",
    "t = 0\n",
    "while t < num_iterations do\n",
    "    \n",
    "  t = t + 1\n",
    "  optim.adam(f, params, config)\n",
    "  --optim.sgd(f, params, config)\n",
    "\n",
    "  -- Check training and validation accuracy once in a while\n",
    "  if t % 200 == 0 then\n",
    "        \n",
    "    local train_acc = check_accuracy(dset.X_train, dset.y_train, model, batch_size)\n",
    "    local val_acc = check_accuracy(dset.X_val, dset.y_val, model, batch_size)\n",
    "    print('train acc: ', train_acc, 'val_acc: ', val_acc)\n",
    "    model:training()\n",
    "        \n",
    "  end\n",
    "\n",
    "  if t % 7500 == 0 then\n",
    "    config.learningRate = config.learningRate / 1.5\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Save the convnet model\n",
    "local function save_model(model, out_file)\n",
    "    \n",
    "  local next_weight_idx = 1\n",
    "  local next_bn_idx = 1\n",
    "    \n",
    "  local f = hdf5.open(out_file, 'w')\n",
    "  for i = 1, #model do\n",
    "        \n",
    "    local layer = model:get(i)\n",
    "    if torch.isTypeOf(layer, nn.SpatialConvolution) or \n",
    "       torch.isTypeOf(layer, nn.Linear) then\n",
    "      f:write(string.format('/W%d', next_weight_idx), layer.weight:float())\n",
    "      f:write(string.format('/b%d', next_weight_idx), layer.bias:float())\n",
    "      next_weight_idx = next_weight_idx + 1\n",
    "            \n",
    "    elseif torch.isTypeOf(layer, nn.SpatialBatchNormalization) or\n",
    "           torch.isTypeOf(layer, nn.BatchNormalization) then\n",
    "      f:write(string.format('/gamma%d', next_bn_idx), layer.weight:float())\n",
    "      f:write(string.format('/beta%d', next_bn_idx), layer.bias:float())\n",
    "      f:write(string.format('/running_mean%d', next_bn_idx), layer.running_mean:float())\n",
    "            \n",
    "      if torch.isTypeOf(layer, nn.BatchNormalization) then\n",
    "        f:write(string.format('/running_var%d', next_bn_idx),\n",
    "                torch.pow(layer.running_std, -2.0):add(-layer.eps):float())\n",
    "                \n",
    "      elseif torch.isTypeOf(layer, nn.SpatialBatchNormalization) then\n",
    "        f:write(string.format('/running_var%d', next_bn_idx),\n",
    "                layer.running_var:float())\n",
    "      end\n",
    "      next_bn_idx = next_bn_idx + 1\n",
    "    end\n",
    "  end\n",
    "  f:close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
